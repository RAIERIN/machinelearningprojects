{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Computer Vision?\n",
    "\n",
    "Computer Vision is the field of study that enables computers to see and identify digital images and videos as a human would. The challenges if faces largely follow from the limited understanding of biological vision. Computer Vision involves acquiring, processing, analyzing, and understanding digital images to extract high-dimensional data from the real world in order to generate symbolic or numerical information which can then be used to make decisions. The process often includes practices like object recognition, video tracking, motion estimation, and image restoration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is OpenCV?\n",
    "\n",
    "OpenCV is short for Open Source Computer Vision. Intuitively by the name, it is an open-source Computer Vision and Machine Learning Library. This library is capable of processing real-time image and video while boosting analytical capabilities. It supports the Deep LEarning frameworks TensorFlow, Caffe, and PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a CNN?\n",
    "A Convolutional Neural Network is a deep neural network (DNN) widely used for the purposes of image recognition and processing and NLP. Also known as ConvNet, a CNN has input and outputl layers and multiple hidden layers, many of which are convolutional. In a way, CNNs are regukarized mutlilayer perceptrons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender and Age Detection \n",
    "To build a gender and age detector that can approximately guess the gender and age of the person in a picture using Deep Learnning on the Adience dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender and Age Detection Project\n",
    "\n",
    "We will use Deep Learning to accurately identify the gender and age of a person from a single image of a face. We will use the models trained by Tal Hassner and Gil Levi. The predicted gender may be one of \"Male\" and \"Female\", and the predicted age may be one of the following ranges - (0-2),(4-6),(8-12),(15-20),(25-32),(38-43),(48-53),(60_100) (8 nodes in the final softmax layer). It is very difficult to accurately guess an exact age from a single image because a factors like makeup, lighting, obstructions, and facial expressions. And so, we make this a classification problem instead of making it one of regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The CNN Architecture\n",
    "The convolutional neural network for this project has 3 convolutional layers:\n",
    "\n",
    "1. Convolutional layer; 96 nodes, kernel size 7\n",
    "2. Convolutional layer; 256 nodes, kernel size 5\n",
    "3. Convolutional layer; 384 nodes, kernel size 3\n",
    "\n",
    "It has 2 fully connected layers, each with 512 nodes and a final output layer of softmax type.\n",
    "\n",
    "We will do following process:\n",
    "1. Detect faces\n",
    "2. Classify into Male/Female\n",
    "3. Classify into one of the 8 age ranges\n",
    "3. Put the results on the image and display it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Dataset\n",
    "For this project, we'll use the Adience dataset; the dataset is available here. This dataset serves as a benchmark for face photos and is inclusive of various real-world imaging conditions like nose, lighting, pose and appearance. The images have been collected from Flickr albums and distributed under Creative Commons(CC) license. It has a total of 26,580 photos of 2,284 subjects in eight age ranges (as mentioned above) and is about 1GB in size. The models we will use have been trained on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install OpenCV\n",
    "\n",
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps for practicing gender and age detection \n",
    "\n",
    "The contents of files are:\n",
    "1. opencv_face_detector.pbtxt\n",
    "2. opencv_face_detector_uint8.pb\n",
    "3. age_deploy.prototxt\n",
    "4. age_net.caffemodel\n",
    "5. gender_deploy.prototxt\n",
    "6. gender_net.caffemodel\n",
    "7. a few pictures to try the project on\n",
    "\n",
    "For face detection, we have a .pb file- this is a protobuff file (protocol buffer); it holds the graph definition and the trained weights of the model. We can use this to run the trained model. And while a .pb file holds the protobuff in binary format, one with the .pbtxt extension holds it in text format. These are TensorFLow files. For age and gender, the  .prototxt files describe the network configuration and the .caffemodel file defines the internal states of the parameters of the layers.\n",
    "\n",
    "1. We use the argparse library to create an argument parser so we can get the image argument from the command prompt. We make it parse the argument holding the path to the image to classify gender and age for.\n",
    "\n",
    "3. For face, age, and gender, initialize protocol buffer and model.\n",
    "\n",
    "4. Initialize the mean values for the model and the lists of age ranges and genders to classify from.\n",
    "\n",
    "5. Now, use the readNet() method to load the networks. The first parameter holds trained weights and the second carries network configuration.\n",
    "\n",
    "6. Let’s capture video stream in case you’d like to classify on a webcam’s stream. Set padding to 20.\n",
    "\n",
    "7. Now until any key is pressed, we read the stream and store the content into the names hasFrame and frame. If it isn’t a video, it must wait, and so we call up waitKey() from cv2, then break.\n",
    "\n",
    "8. Let’s make a call to the highlightFace() function with the faceNet and frame parameters, and what this returns, we will store in the names resultImg and faceBoxes. And if we got 0 faceBoxes, it means there was no face to detect.\n",
    "Here, net is faceNet- this model is the DNN Face Detector and holds only about 2.7MB on disk.\n",
    "\n",
    "    a. Create a shallow copy of frame and get its height and width.\n",
    "    b. Create a blob from the shallow copy.\n",
    "    c.Set the input and make a forward pass to the network.\n",
    "    d.faceBoxes is an empty list now. for each value in 0 to 127, define the         confidence (between 0 and 1). Wherever we find the confidence greater           than the confidence threshold, which is 0.7, we get the x1, y1, x2, and         y2 coordinates and append a list of those to faceBoxes.\n",
    "    e.Then, we put up rectangles on the image for each such list of coordinates and return two things: the shallow copy and the list of faceBoxes.\n",
    "9. But if there are indeed faceBoxes, for each of those, we define the face, create a 4-dimensional blob from the image. In doing this, we scale it, resize it, and pass in the mean values.\n",
    "\n",
    "10. We feed the input and give the network a forward pass to get the confidence of the two class. Whichever is higher, that is the gender of the person in the picture.\n",
    "\n",
    "11. Then, we do the same thing for age.\n",
    "\n",
    "12. We’ll add the gender and age texts to the resulting image and display it with imshow()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-TF2.0] *",
   "language": "python",
   "name": "conda-env-py3-TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
