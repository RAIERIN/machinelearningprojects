{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Speech emotion recognition, the best example of it can be seen at call centers. If you ever noticed, call centers employees never talk in the same manner, their way of pitching/talking to the customers changes with customers. Now, this does happen in common people too, but how is this relevant to call centers? Here is your answer, the employees recognize customers' emotions from speech, so they can improve their service and convert more people. In this way, they are using speech emotion recognition. So, let's discuss this project in detail.\n",
    "\n",
    "Speech emotion recognition is a simple Python mini-project, which you are going to practice with DataFlair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Speech Emotion Recognition?\n",
    "\n",
    "Speech Emotion Recognition, abbreviated as SER, is the act of attempting to recognize human emotion and affective states from speech. This is capitalizing on the fact that voice often reflects underlying emotion through tone and pitch. This is also the phenomenon that animals like dogs and horses employ to be able to understand human emotion.\n",
    "\n",
    "SER is tough because emotions are subjective and annotating audio challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Librosa?\n",
    "\n",
    "Librosa is a Python Library for analyzing audio and music. It has a flatter pacakge layout, standardizes interfaces and names, backward compatibility, modular functions and readabale code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is JupyterLab?\n",
    "\n",
    "JupyterLab is an open-source, web-based UI for Project Jupyter and it has basic functionalities of the Jupyter Notebook, like notebooksm terminals, text editors, file browsers, rich outputs and more. However, it also provides improved support for third party extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Emotion Recognition - Objective\n",
    "To build a model to recognize emotion from speech using the librosa and sklearn libraries and the RAVDESS dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Emotion Recognition - About the Project\n",
    "\n",
    "In this project, we will use the libraries librosa, soundfile, and sklearn to build a model using an MLPClassifier. This will be able to recognize emotion from sound files. We will load the data, extract features from it, then split the dataset into training and testing sets. Then, we'll initialize an MLPCLassifier and traing the model. Finally, we'll calculate the accuracy of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Dataset\n",
    "For this Python project, we'll use the RAVDESS dataset; this is the Ryerson Audio-Visual Database of Emotional Speech and Song dataset, and is free to download. This dataset has 7356 files rated by 247 individuals 10 times on emotonal validity, intensity and genuineness. The entire dataset is 24.8 GB from 24 actors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "pip install librosa soundfile numpy sklearn pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary Imports\n",
    "import librosa\n",
    "import soundfile\n",
    "import os, glob, pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define a function extract_feature to extract the mfcc, chroma, and mel features from a sound file. This function takes 4 parameters - the file name and three Boolean parameters for the three features:\n",
    "\n",
    "a. mfcc: Mel Frequency Cepstral Coefficient, represents the short-term power spectrum of a sound\n",
    "b. chroma: Pertains to the 12 different pitch classes\n",
    "c. mel: Mel Spectrogram Frequency\n",
    "\n",
    "Open the sound file with soundfile. SoundFile using with-as so it's automatically closed once we're done. Read from it and call it X. Also, get the sample rate. If chroma is True, get the Short-TIme Fourier Transform of X.\n",
    "\n",
    "Let reuslt be an empty numpy array. Now, for each feature of the three, if it exists, make a call to the corresponding funtion from librosa.feature(eg-librosa.feature.mfcc for mfcc), and get the mean value. Call the function hstack() from numpy with result and the feature value, and store this in result. hstack() stacks array in sequential horizontally (in a columnar fashion). Then, return the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract features (mfcc, chroma, mel) from a sound file\n",
    "def extract_feature(file_name, mfcc, chroma, mel):\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        sample_rate=sound_file.samplerate\n",
    "        if chroma:\n",
    "            stft=np.abs(librosa.stft(X))\n",
    "        result=np.array([])\n",
    "        if mfcc:\n",
    "            mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "            result=np.hstack((result, mfccs))\n",
    "        if chroma:\n",
    "            chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "            result=np.hstack((result, chroma))\n",
    "        if mel:\n",
    "            mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "            result=np.hstack((result, mel))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define a dictionary to hold numbers and the emotions available in the RAVDESS dataset, and a list to hold those we want-calm, happy, fearful,disgust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emotions in the RAVDESS dataset\n",
    "emotions={\n",
    "  '01':'neutral',\n",
    "  '02':'calm',\n",
    "  '03':'happy',\n",
    "  '04':'sad',\n",
    "  '05':'angry',\n",
    "  '06':'fearful',\n",
    "  '07':'disgust',\n",
    "  '08':'surprised'\n",
    "}\n",
    "#DataFlair - Emotions to observe\n",
    "observed_emotions=['calm', 'happy', 'fearful', 'disgust']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load the data with a function load_data() - this takes in the relative size of the test set as parameter. x and y are empty lists; we'll use the glob() function from the glob module to get all the pathnames for the sound file in our dataset. The patternwe use for this is: \"D:\\\\DataFlair\\\\ravdessdata\\\\Actor_*\\\\*.wav\". So, for each such path, get the basename of the file, the emotion by splitting the name around ‘-’ and extracting the third value:\n",
    "\n",
    "Using our emotion dictionary, this number is turned into an emotion, and our function checks wheteher this emotion is in our list of observed_emotions; if not, it continues to the next file. It makes a call to extract_feature and stores what is returned in 'feature'. Then, it appends the feature to x and the emotion to y. So, the list x holds the features and y holds the emotions. We call the function train_test_split with these, the test size and  a random state value and return that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data and extract features for each sound file\n",
    "def load_data(test_size=0.2):\n",
    "    x,y=[],[]\n",
    "    for file in glob.glob(\"C:\\\\Users\\\\Ace\\\\Desktop\\\\MachineLearning\\\\machinelearningprojects\\\\Speech Recognition with librosa\\\\ravdessdata\\\\Actor_*\\\\*.wav\"):\n",
    "        file_name=os.path.basename(file)\n",
    "        emotion=emotions[file_name.split(\"-\")[2]]\n",
    "        if emotion not in observed_emotions:\n",
    "            continue\n",
    "        feature=extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "        x.append(feature)\n",
    "        y.append(emotion)\n",
    "    return train_test_split(np.array(x), y, test_size=test_size, random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to split the dataset into training and testing sets! lets keep the test set 25% of everything and use the load_data functin for this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset\n",
    "x_train, x_test, y_train, y_test = load_data(test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the shape of the training and testing datasets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(576, 192)\n"
     ]
    }
   ],
   "source": [
    "#Get the shape of the training and testing datasets\n",
    "print((x_train.shape[0],x_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted: 180\n"
     ]
    }
   ],
   "source": [
    "#Get the number of features extracted\n",
    "print(f'Features extracted: {x_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's initialize an MLPClassifier. This is Multi-layer Perceptron Classifier; it optimizes the log-loss funtion using LBFGS or stochastic gradient descent. Unlike SVM or Naive Bayes, the MLPClassifier has an internal neutral network for the purpose of classification. This is a feedforward ANN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFlair - Initialize the Multi Layer Perceptron Classifier\n",
    "model = MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08,hidden_layer_sizes=(300,),learning_rate=\"adaptive\",max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.01, batch_size=256, beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(300,), learning_rate='adaptive',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=500,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for the test set\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy of our model\n",
    "ac"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-TF2.0] *",
   "language": "python",
   "name": "conda-env-py3-TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
